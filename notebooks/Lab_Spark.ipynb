{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d40bd50",
   "metadata": {},
   "source": [
    "# RDD Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16333332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "import time\n",
    "import kagglehub\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
    "print(\"Dataset path:\", path)\n",
    "\n",
    "# Load data\n",
    "text_rdd = sc.textFile(f\"{path}/train.ft.txt\")  # Adjust path as needed\n",
    "\n",
    "# Word Count without caching\n",
    "start_time = time.time()\n",
    "word_counts_no_cache = (text_rdd\n",
    "                      .flatMap(lambda line: line.split(\" \"))\n",
    "                      .map(lambda word: (word.lower(), 1))\n",
    "                      .reduceByKey(lambda a, b: a + b))\n",
    "word_counts_no_cache.count()\n",
    "no_cache_time = time.time() - start_time\n",
    "\n",
    "# Word Count with caching\n",
    "start_time = time.time()\n",
    "words_rdd = (text_rdd\n",
    "             .flatMap(lambda line: line.split(\" \"))\n",
    "             .map(lambda word: (word.lower(), 1))\n",
    "             .persist(StorageLevel.MEMORY_ONLY))\n",
    "\n",
    "word_counts_cache = words_rdd.reduceByKey(lambda a, b: a + b)\n",
    "word_counts_cache.count()\n",
    "cache_time = time.time() - start_time\n",
    "\n",
    "# Results\n",
    "print(f\"Time without caching: {no_cache_time:.2f} seconds\")\n",
    "print(f\"Time with caching: {cache_time:.2f} seconds\")\n",
    "print(f\"Performance improvement: {(no_cache_time - cache_time)/no_cache_time*100:.1f}%\")\n",
    "\n",
    "# Show top 10 words\n",
    "word_counts_cache.takeOrdered(10, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72d66e3",
   "metadata": {},
   "source": [
    "# Dataframe Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e1e8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define schema for reviews\n",
    "schema = StructType([\n",
    "    StructField(\"label\", StringType(), True),\n",
    "    StructField(\"review\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame from text file\n",
    "df = spark.read.csv(f\"{path}/train.ft.txt\", sep=\"\\t\", schema=schema)\n",
    "\n",
    "# Add length column\n",
    "df = df.withColumn(\"review_length\", F.length(\"review\"))\n",
    "\n",
    "# RDD operation - calculate average review length by label\n",
    "start_time = time.time()\n",
    "rdd_result = (df.rdd\n",
    "              .map(lambda row: (row.label, (row.review_length, 1)))\n",
    "              .reduceByKey(lambda a, b: (a[0]+b[0], a[1]+b[1])))\n",
    "              .mapValues(lambda x: x[0]/x[1])))\n",
    "rdd_time = time.time() - start_time\n",
    "\n",
    "# DataFrame operation - same calculation\n",
    "start_time = time.time()\n",
    "df_result = (df.groupBy(\"label\")\n",
    "             .agg(F.avg(\"review_length\").alias(\"avg_length\")))\n",
    "df_time = time.time() - start_time\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(['RDD', 'DataFrame'], [rdd_time, df_time], color=['blue', 'green'])\n",
    "plt.title('Execution Time Comparison')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.show()\n",
    "\n",
    "# Show results\n",
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51cade1",
   "metadata": {},
   "source": [
    "# Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5810a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporary view\n",
    "df.createOrReplaceTempView(\"amazon_reviews\")\n",
    "\n",
    "# SQL query for sentiment analysis (assuming __label__1 is positive, __label__2 is negative)\n",
    "sql_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN label = '__label__1' THEN 'positive'\n",
    "            ELSE 'negative'\n",
    "        END as sentiment,\n",
    "        AVG(review_length) as avg_length,\n",
    "        COUNT(*) as review_count\n",
    "    FROM amazon_reviews\n",
    "    GROUP BY sentiment\n",
    "\"\"\")\n",
    "\n",
    "# Show results\n",
    "sql_result.show()\n",
    "\n",
    "# Combined approach using DataFrame API\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "df_with_sentiment = df.withColumn(\n",
    "    \"sentiment\", \n",
    "    when(df.label == \"__label__1\", \"positive\").otherwise(\"negative\")\n",
    ")\n",
    "\n",
    "analysis_result = (df_with_sentiment\n",
    "                 .groupBy(\"sentiment\")\n",
    "                 .agg(\n",
    "                     F.avg(\"review_length\").alias(\"avg_length\"),\n",
    "                     F.count(\"*\").alias(\"review_count\")\n",
    "                 ))\n",
    "\n",
    "# Compare performance\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        CASE WHEN label = '__label__1' THEN 'positive' ELSE 'negative' END as sentiment,\n",
    "        AVG(review_length) as avg_length\n",
    "    FROM amazon_reviews\n",
    "    GROUP BY sentiment\n",
    "\"\"\").collect()\n",
    "sql_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "(df.withColumn(\"sentiment\", when(df.label == \"__label__1\", \"positive\").otherwise(\"negative\"))\n",
    " .groupBy(\"sentiment\")\n",
    " .agg(F.avg(\"review_length\"))\n",
    " .collect())\n",
    "df_api_time = time.time() - start_time\n",
    "\n",
    "print(f\"SQL execution time: {sql_time:.4f} seconds\")\n",
    "print(f\"DataFrame API execution time: {df_api_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5593ba",
   "metadata": {},
   "source": [
    "# Optimization and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037dd79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configure Spark for optimization\n",
    "spark = (SparkSession.builder\n",
    "         .appName(\"AmazonReviewsAnalysis\")\n",
    "         .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "         .config(\"spark.executor.memory\", \"2g\")\n",
    "         .getOrCreate())\n",
    "\n",
    "# 1. Selective persistence\n",
    "optimized_df = (df.filter(F.length(\"review\") > 0)  # Remove empty reviews\n",
    "               .select(\"label\", \"review\", \"review_length\")\n",
    "               .persist(StorageLevel.MEMORY_AND_DISK))\n",
    "\n",
    "# 2. Strategic partitioning\n",
    "(optimized_df.repartition(4, \"label\")\n",
    "            .write\n",
    "            .mode(\"overwrite\")\n",
    "            .parquet(\"data/amazon_reviews_optimized\"))\n",
    "\n",
    "# 3. Reading optimized data\n",
    "optimized_reviews = spark.read.parquet(\"data/amazon_reviews_optimized\")\n",
    "\n",
    "# Show partitioning information\n",
    "print(f\"Number of partitions: {optimized_reviews.rdd.getNumPartitions()}\")\n",
    "optimized_reviews.groupBy(\"label\").count().show()\n",
    "\n",
    "# Example optimized analysis\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover\n",
    "\n",
    "# Tokenize reviews\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "words_df = tokenizer.transform(optimized_reviews)\n",
    "\n",
    "# Remove stop words\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "cleaned_df = remover.transform(words_df)\n",
    "\n",
    "# Show results\n",
    "cleaned_df.select(\"label\", \"filtered_words\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa43e27",
   "metadata": {},
   "source": [
    "# Comparative Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e031ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "# Create comparison table\n",
    "comparison_table = PrettyTable()\n",
    "comparison_table.field_names = [\"Operation\", \"RDD\", \"DataFrame\", \"Spark SQL\"]\n",
    "\n",
    "comparison_table.add_row([\"Word Count\", f\"{no_cache_time:.2f}s\", \"N/A\", \"N/A\"])\n",
    "comparison_table.add_row([\"Word Count (cached)\", f\"{cache_time:.2f}s\", \"N/A\", \"N/A\"])\n",
    "comparison_table.add_row([\"Aggregation\", f\"{rdd_time:.2f}s\", f\"{df_time:.2f}s\", f\"{sql_time:.2f}s\"])\n",
    "comparison_table.add_row([\"Sentiment Analysis\", \"N/A\", f\"{df_api_time:.2f}s\", f\"{sql_time:.2f}s\"])\n",
    "\n",
    "print(\"Performance Comparison:\")\n",
    "print(comparison_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c6c0fa",
   "metadata": {},
   "source": [
    "# Additional Analysis Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c242c374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment word frequency analysis\n",
    "from collections import defaultdict\n",
    "\n",
    "# Get positive and negative reviews\n",
    "positive_reviews = cleaned_df.filter(cleaned_df.label == \"__label__1\")\n",
    "negative_reviews = cleaned_df.filter(cleaned_df.label == \"__label__2\")\n",
    "\n",
    "# Function to count words\n",
    "def count_words(df):\n",
    "    word_counts = defaultdict(int)\n",
    "    for row in df.select(\"filtered_words\").collect():\n",
    "        for word in row.filtered_words:\n",
    "            word_counts[word] += 1\n",
    "    return word_counts\n",
    "\n",
    "# Count words in positive and negative reviews\n",
    "positive_counts = count_words(positive_reviews)\n",
    "negative_counts = count_words(negative_reviews)\n",
    "\n",
    "# Find most distinctive words\n",
    "distinctive_words = []\n",
    "for word in set(positive_counts.keys()).union(set(negative_counts.keys())):\n",
    "    pos = positive_counts.get(word, 0)\n",
    "    neg = negative_counts.get(word, 0)\n",
    "    if pos + neg > 100:  # Only consider words with significant occurrence\n",
    "        ratio = (pos + 1) / (neg + 1)  # Add 1 to avoid division by zero\n",
    "        distinctive_words.append((word, ratio, pos, neg))\n",
    "\n",
    "# Top 10 positive words\n",
    "sorted(distinctive_words, key=lambda x: -x[1])[:10]\n",
    "\n",
    "# Top 10 negative words\n",
    "sorted(distinctive_words, key=lambda x: x[1])[:10]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
